{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Descent: Step-by-Step Mathematical Example**  \n",
    "Gradient Descent is an **iterative optimization algorithm** used to minimize the error in Linear Regression by adjusting the parameters \\( m \\) (slope) and \\( b \\) (intercept) step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Problem Statement**  \n",
    "We have a small dataset:\n",
    "\n",
    "| Hours Studied (\\( X \\)) | Exam Score (\\( Y \\)) |\n",
    "|------------------|------------------|\n",
    "| 1 | 50 |\n",
    "| 2 | 55 |\n",
    "| 3 | 65 |\n",
    "\n",
    "We assume that the relationship between \\( X \\) and \\( Y \\) follows the equation:\n",
    "\n",
    "\\[\n",
    "Y = mX + b\n",
    "\\]\n",
    "\n",
    "Our goal is to find the **best values of \\( m \\) and \\( b \\)** using **Gradient Descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Define the Cost Function**\n",
    "We use **Mean Squared Error (MSE)** as the cost function:\n",
    "\n",
    "\\[\n",
    "J(m, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (Y_i - \\hat{Y}_i)^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( m \\) = Number of training samples (**not slope**)\n",
    "- \\( Y_i \\) = Actual target value\n",
    "- \\( \\hat{Y}_i \\) = Predicted value (\\( mX_i + b \\))\n",
    "\n",
    "We use **\\( \\frac{1}{2m} \\)** instead of \\( \\frac{1}{m} \\) to simplify differentiation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Compute Partial Derivatives**\n",
    "To minimize the cost function, we compute **gradients (partial derivatives)** of \\( J(m, b) \\):\n",
    "\n",
    "1. **Gradient w.r.t \\( m \\) (Slope update rule)**:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial J}{\\partial m} = -\\frac{1}{m} \\sum_{i=1}^{m} X_i (Y_i - \\hat{Y}_i)\n",
    "\\]\n",
    "\n",
    "2. **Gradient w.r.t \\( b \\) (Intercept update rule)**:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial J}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^{m} (Y_i - \\hat{Y}_i)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Gradient Descent Algorithm**\n",
    "We update \\( m \\) and \\( b \\) iteratively using:\n",
    "\n",
    "\\[\n",
    "m = m - \\alpha \\cdot \\frac{\\partial J}{\\partial m}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\alpha \\) = **Learning rate** (controls step size)\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Step-by-Step Example Calculation**\n",
    "### **Step 1: Initialize Values**\n",
    "We start with:\n",
    "- \\( m = 0 \\), \\( b = 0 \\)\n",
    "- Learning rate \\( \\alpha = 0.01 \\)\n",
    "\n",
    "### **Step 2: Compute Predictions**\n",
    "For each \\( X \\):\n",
    "\n",
    "\\[\n",
    "\\hat{Y} = mX + b\n",
    "\\]\n",
    "\n",
    "Since **initially \\( m = 0 \\) and \\( b = 0 \\):**\n",
    "- \\( \\hat{Y}_1 = 0(1) + 0 = 0 \\)\n",
    "- \\( \\hat{Y}_2 = 0(2) + 0 = 0 \\)\n",
    "- \\( \\hat{Y}_3 = 0(3) + 0 = 0 \\)\n",
    "\n",
    "### **Step 3: Compute Gradients**\n",
    "**Compute \\( \\frac{\\partial J}{\\partial m} \\):**\n",
    "\\[\n",
    "\\frac{\\partial J}{\\partial m} = -\\frac{1}{3} [(1(50 - 0)) + (2(55 - 0)) + (3(65 - 0))]\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= -\\frac{1}{3} [(50) + (110) + (195)]\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= -\\frac{1}{3} (355) = -118.33\n",
    "\\]\n",
    "\n",
    "**Compute \\( \\frac{\\partial J}{\\partial b} \\):**\n",
    "\\[\n",
    "\\frac{\\partial J}{\\partial b} = -\\frac{1}{3} [(50 - 0) + (55 - 0) + (65 - 0)]\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= -\\frac{1}{3} (50 + 55 + 65) = -\\frac{1}{3} (170) = -56.67\n",
    "\\]\n",
    "\n",
    "### **Step 4: Update Parameters**\n",
    "Using \\( \\alpha = 0.01 \\):\n",
    "\n",
    "\\[\n",
    "m = 0 - (0.01 \\times -118.33) = 1.1833\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "b = 0 - (0.01 \\times -56.67) = 0.5667\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Next Iteration**\n",
    "Using updated values \\( m = 1.1833 \\), \\( b = 0.5667 \\), we repeat the process:\n",
    "\n",
    "1. Compute predictions\n",
    "2. Compute gradients\n",
    "3. Update parameters\n",
    "\n",
    "After **multiple iterations**, \\( m \\) and \\( b \\) will converge to the **optimal values**.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Final Result**\n",
    "After multiple iterations, the algorithm converges to:\n",
    "\n",
    "\\[\n",
    "m \\approx 7.5, \\quad b \\approx 42\n",
    "\\]\n",
    "\n",
    "So, the **best-fit line** is:\n",
    "\n",
    "\\[\n",
    "Y = 7.5X + 42\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Python Implementation**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Dataset\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([50, 55, 65])\n",
    "\n",
    "# Initialize parameters\n",
    "m = 0\n",
    "b = 0\n",
    "alpha = 0.01  # Learning rate\n",
    "epochs = 1000  # Number of iterations\n",
    "n = len(X)  # Number of data points\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for _ in range(epochs):\n",
    "    Y_pred = m * X + b  # Predictions\n",
    "    dm = (-2/n) * sum(X * (Y - Y_pred))  # Derivative w.r.t m\n",
    "    db = (-2/n) * sum(Y - Y_pred)  # Derivative w.r.t b\n",
    "    m = m - alpha * dm  # Update m\n",
    "    b = b - alpha * db  # Update b\n",
    "\n",
    "# Print final values\n",
    "print(\"Final Slope (m):\", round(m, 5))\n",
    "print(\"Final Intercept (b):\", round(b, 5))\n",
    "\n",
    "# Make a prediction for 3 hours of study\n",
    "print(\"Prediction for 3 hours of study:\", round(m * 3 + b, 2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Key Takeaways**\n",
    "âœ… **Gradient Descent is an iterative approach** to optimize \\( m \\) and \\( b \\).  \n",
    "âœ… The **cost function (MSE) measures how good our model is**.  \n",
    "âœ… **Partial derivatives (gradients) guide parameter updates**.  \n",
    "âœ… **Learning rate \\( \\alpha \\) controls step size** (too high = unstable, too low = slow).  \n",
    "âœ… After multiple iterations, the **best line is found**!  \n",
    "\n",
    "Would you like me to plot the **Gradient Descent visualization**? ðŸ“‰ðŸ”¥\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
